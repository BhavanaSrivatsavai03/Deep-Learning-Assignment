# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nmx7cV6KhuIZBv85Venz33vlBNP_ZHNl
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

def create_model_DNN_A():
    model = Sequential([
        Dense(64, activation='relu', input_shape=(784,)),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

model_A = create_model_DNN_A()
model_A.summary()

def create_model_DNN_B():
    model = Sequential([
        Dense(256, activation='relu', input_shape=(784,)),
        Dense(512, activation='relu'),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

model_B = create_model_DNN_B()
model_B.summary()

from tensorflow.keras.datasets import mnist

# Correctly formatted file path
path_to_zip = r'C:/content/sample_data/mnist_dl/archive.zip'  # Use raw string or forward slashes

# Load the dataset using Keras's built-in function
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data as shown before
X_train = X_train.reshape(-1, 784).astype('float32') / 200
X_test = X_test.reshape(-1, 784).astype('float32') / 200

from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Load and preprocess MNIST data
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Flatten the 28x28 images into 784-dimensional vectors and normalize the pixel values to [0, 1]
X_train = X_train.reshape(-1, 784).astype('float32') / 200
X_test = X_test.reshape(-1, 784).astype('float32') / 200

# One-hot encode
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

def create_model_DNN_A():
    model = Sequential()
    model.add(Dense(64, input_shape=(784,), activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

def create_model_DNN_B():
    model = Sequential()
    model.add(Dense(256, input_shape=(784,), activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

model_A = create_model_DNN_A()
model_B = create_model_DNN_B()

model_A.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
model_B.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

history_A = model_A.fit(X_train, y_train, epochs=500, batch_size=64, validation_data=(X_test, y_test), verbose=1)
history_B= model_B.fit(X_train, y_train, epochs=500, batch_size=64, validation_data=(X_test, y_test), verbose=1)

test_loss_1, test_acc_1 = model_A.evaluate(X_test, y_test, verbose=0)
test_loss_2, test_acc_2 = model_B.evaluate(X_test, y_test, verbose=0)

print(f"Model A Test Accuracy: {test_acc_1:.4f}")
print(f"Model B Test Accuracy: {test_acc_2:.4f}")

# Plot the training and validation accuracy over epochs
def plot_learning_curves(history, title):
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_learning_curves(history_A, "Model A Accuracy")
plot_learning_curves(history_B, "Model B Accuracy")

import numpy as np
def plot_training_error(history, model_name):
    training_accuracy = history.history['accuracy']
    if not training_accuracy:
        print(f"No accuracy data found for model {model_name}")
        return

    training_error = 1 - np.array(training_accuracy)
    plt.plot(training_error, label='Training Error')
    plt.title(f'Model {model_name} Training Error vs Time')
    plt.ylabel('Training Error')
    plt.xlabel('Epoch')
    plt.legend(loc='upper right')
    plt.show()

# Call the plotting function
plot_training_error(history_A, 'A')
plot_training_error(history_B, 'B')

import time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(-1, 784).astype('float32') / 200
X_test = X_test.reshape(-1, 784).astype('float32') / 200

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define models
def create_model_DNN_A():
    model = Sequential([
        Dense(64, input_shape=(784,), activation='relu'),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def create_model_DNN_B():
    model = Sequential([
        Dense(256, input_shape=(784,), activation='relu'),
        Dense(512, activation='relu'),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model_A = create_model_DNN_A()
model_B = create_model_DNN_B()

# Function to train and record metrics
def train_and_record_metrics(model, X_train, y_train, X_test, y_test, epochs=500, batch_size=64):
    history = {'train_loss': [], 'val_loss': [], 'time': []}
    start_time = time.time()

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        history_epoch = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0, validation_data=(X_test, y_test))

        if history_epoch.history:
            print(f"Train Loss: {history_epoch.history['loss'][0]}")
            print(f"Validation Loss: {history_epoch.history['val_loss'][0]}")
            history['train_loss'].append(history_epoch.history['loss'][0])
            history['val_loss'].append(history_epoch.history['val_loss'][0])
        else:
            print("History is empty!")

        elapsed_time = time.time() - start_time
        history['time'].append(elapsed_time)

    return history

try:
    metrics_A = train_and_record_metrics(model_A, X_train, y_train, X_test, y_test)
    metrics_B = train_and_record_metrics(model_B, X_train, y_train, X_test, y_test)
except Exception as e:
    print(f"An error occurred: {e}")

# Plotting function
def plot_error_vs_time(metrics_A, metrics_B):
    plt.figure(figsize=(12, 6))
    plt.plot(metrics_A['time'], metrics_A['train_loss'], label='Model A Training Error', color='blue')
    plt.plot(metrics_A['time'], metrics_A['val_loss'], label='Model A Validation Error', color='green')
    plt.plot(metrics_B['time'], metrics_B['train_loss'], label='Model B Training Error', color='red')
    plt.plot(metrics_B['time'], metrics_B['val_loss'], label='Model B Validation Error', color='black')
    plt.xlabel('Training Time (seconds)')
    plt.ylabel('Error (Loss)')
    plt.title('Training and Validation Error vs. Training Time')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.show()

# Call the plotting function
plot_error_vs_time(metrics_A, metrics_B)

import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(-1, 784).astype('float32') / 200
X_test = X_test.reshape(-1, 784).astype('float32') / 200
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

def create_model_DNN_A():
    model = Sequential()
    model.add(Dense(64, input_shape=(784,), activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

def create_model_DNN_B():
    model = Sequential()
    model.add(Dense(256, input_shape=(784,), activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    return model

# Compile models
model_A = create_model_DNN_A()
model_B = create_model_DNN_B()

model_A.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_B.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_A = model_A.fit(X_train, y_train, epochs=500, batch_size=64, validation_data=(X_test, y_test), verbose=1)
history_B = model_B.fit(X_train, y_train, epochs=500, batch_size=64, validation_data=(X_test, y_test), verbose=1)

def plot_loss_vs_epochs(history, model_name):
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Testing Loss')
    plt.title(f'{model_name} - Loss vs. Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_loss_vs_epochs(history_A, "Model A")
plot_loss_vs_epochs(history_B, "Model B")